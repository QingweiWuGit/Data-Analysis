{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9334658-6ec8-4fe1-be9e-d22a2fe070a0",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "* LR的变量筛选主要从稳定性、重要性、相关性和解释性这几个维度下手。\n",
    "  因为没有更多跨时代和跨渠道的样本，稳定性暂时没有考虑\n",
    "## 特征筛选\n",
    "* 通过IV值来衡量变量的重要性。IV < 0.02说明单变量区分能力较弱，可以筛选掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7392950-85ec-4dbc-95d0-2d3fa9db7068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discretize(data, columns_continous, quantiles):\n",
    "    '''\n",
    "    等频分箱函数\n",
    "    input:\n",
    "    data: dataframe, 原始变量数据\n",
    "    columns_continous: list， 连续性变量列表\n",
    "    quantiles: list, 等频分箱的分位点列表\n",
    "    return:\n",
    "    data_bin: dataframe, 分箱后的数据，每箱为字符型\n",
    "    '''\n",
    "    data_bin = data.copy()\n",
    "    columns_cate = [column for column in data_bin.columns if column not in columns_continous]\n",
    "    for column in columns_continous:\n",
    "        X = data_bin[column].copy()\n",
    "        for i in range(len(quantiles)-1):\n",
    "            left = X.quantile(quantiles[i])\n",
    "            right = X.quantile(quantiles[i+1])\n",
    "            if i<len(quantiles)-2:\n",
    "                group = '[' + str(left) + ',' + str(right) + ')'\n",
    "                data_bin[column].iloc[np.where((X>=left)&(X<right))] = group\n",
    "            if i==len(quantiles)-2:\n",
    "                group = '[' + str(left) + ',' + str(right) + ']'\n",
    "                data_bin[column].iloc[np.where((X>=left)&(X<=right))] = group\n",
    "        data_bin[column].fillna('nan', inplace=True)\n",
    "    for column in columns_cate:\n",
    "        data_bin[column] = data_bin[column].astype(str)\n",
    "    return data_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc118d6-f966-4d48-a702-baa271200072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def woe_iv_calc(data_bin, y):\n",
    "    '''\n",
    "    计算WOE和IV函数\n",
    "    input:\n",
    "    data_bin: dataframe，分箱后的函数\n",
    "    y: series，目标变量，值为0或1\n",
    "    return:\n",
    "    data_woe: dataframe，WOE映射后的数据\n",
    "    map_woe: dict, key为变量名，value为每个箱对应的WOE值\n",
    "    map_iv: dict, key为变量名，value为每个箱对应的IV值\n",
    "    '''\n",
    "    data_woe = data_bin.copy()\n",
    "    map_woe, map_iv = {}, {}\n",
    "    for column in data_woe.columns:\n",
    "        cross = pd.crosstab(data_woe[column], y)\n",
    "        cross[cross==0] = 1 # 解决分母为0的问题\n",
    "        cross = cross/cross.sum(axis=0)\n",
    "        woe = np.log(cross[0]/cross[1])\n",
    "        iv = ((cross[0]-cross[1])*np.log(cross[0]/cross[1])).sum()\n",
    "        map_woe[column] = dict(woe)\n",
    "        map_iv[column] = iv\n",
    "        data_woe[column] = data_woe[column].map(dict(woe))\n",
    "        return data_woe, map_woe, map_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab43c76f-aaaa-4fd6-a347-7a091c62c8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 调用函数\n",
    "# X_columns = data_lr.columns[2:]\n",
    "# Y_columns = 'TARGET'\n",
    "# columns_continuous = eda_stat[eda_stat['count_unique'] > 10].index.tolist()\n",
    "# columns_continuous = [column for column in columns_continuous if column != 'SK_ID_CURR']\n",
    "# quantiles = [0.1*i for i in range(11)]\n",
    "# data_bin = discretize(data_lr[X_columns], columns_continous, quantiles)\n",
    "# data_woe, map_woe, map_iv = woe_iv_calc(data_bin, data_lr[Y_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d45739-b9da-443e-b67f-37d444f4f9c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 变量的相关性包括单变量之间的相关性和多变量之间的多重共线性，这两类相关性过高都会导致线性模型系数不置信。\n",
    "\n",
    "* 相关性通常利用皮尔逊相关系数来衡量，如果系数大于0.8，则认为两个变量强相关，则只需要保留其中IV值较高的那个变量。\n",
    "\n",
    "* 多重共线性主要通过VIF来计算，VIF > 10说明该变量和其它变量存在线性关系，应该将该变量筛掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18126e65-99df-4dc6-91a9-1230c53b656d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# # 计算相关性\n",
    "# data_cor = data_lr[columns_select].corr().abs()\n",
    "# data_cor_lower = pd.DataFrame(np.tril(data_cor), index=data_cor.index, columns=data_cor.columns)\n",
    "\n",
    "# columns_drop = []\n",
    "# for column in data_cor_lower:\n",
    "#     data_cor_select = pd.DataFrame(data_cor_lower.ix[(data_cor_lower[column]>0.8)&(data_cor_lower[column]<1), column])\n",
    "#     if len(data_cor_select) > 0:\n",
    "#         data_cor_select = pd.DataFrame(data=data_cor_select.columns.tolist() + data_cor_select.index.tolist(), columns=['column_name'])\n",
    "#         data_cor_select['IV'] = data_cor_select['column_name'] = map(map_iv)\n",
    "#         data_cor_select = data_cor_select.sort_values(by='IV', ascending=False)\n",
    "#         columns_crop = columns_crop + data_cor_select['column_name'].tolist()[1:]\n",
    "\n",
    "# columns_select = [column for column in columns_select if column not in columns_drop]\n",
    "# data_lr = data_lr[['SK_ID_CURR', 'TARGET'] + columns_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533612df-f9d9-467d-ab32-de5a62ab2360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 计算多重共线性\n",
    "# data_vif = data_lr.ix[:, 2:].copy()\n",
    "# data_vif = sm.add_constant(data_vif)\n",
    "# data_vif = data_vif.replace([np.nan, np.inf], -9999)\n",
    "\n",
    "# vif_select = pd.DataFrame(data=data_vif.columns, columns=['column_name'])\n",
    "# vif_select['VIF'] = [variance_inflation_factor(data_vif.values, i) for i in range(data_vif.shape[1])]\n",
    "\n",
    "# columns_select = vif_select.ix[vif_select['VIF']<10, 'column_name'].tolist()\n",
    "# columns_select = [column for column in columns_select if 'const' not in column]\n",
    "# data_lr = data_lr[['SK_ID_CURR', 'TARGET'] + columns_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b158303-8e9c-4f45-b5ca-6084a002ef8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def woe_plot(map_woe, close=True, show_last=True):\n",
    "    '''\n",
    "    WOE值分布图\n",
    "    input:\n",
    "    map_woe: dict, key为变量名， value为每箱对应的WOE值，建议每箱预先排序方便观察单调性\n",
    "    close: bool，是否打印WOE值分布图，默认为True\n",
    "    show_last: bool, 是否只保留最后一个变量的WOE值分布图，默认为True\n",
    "    \n",
    "    return:\n",
    "    result: dict, key为变量名，value为每个变量的WOE值分布图\n",
    "    '''\n",
    "    result = {}\n",
    "    for i, feature in enumerate(map_woe):\n",
    "        data = pd.Series(map_woe[feature])\n",
    "        data.index.name = ''\n",
    "        data.name = ''\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        data.plot(kind='bar', ax=ax)\n",
    "        ax.set_xlabel('变量分箱')\n",
    "        ax.set_ylabel('WOE值')\n",
    "        ax.set_title('%s' %feature)\n",
    "        result[feature] = fig\n",
    "        if close and show_last and i<len(map_woe)-1:\n",
    "            plt.clost('all')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a0ec777-4f26-4f9a-9a2e-339f32e747c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 训练模型\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# data_lr[columns_select] = data_woe[columns_select]\n",
    "# X_columns = data_lr.columns[2:]\n",
    "# Y_columns = 'TARGET'\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_lr[X_columns], data_lr[Y_columns], test_size=0.3, random_state=0)\n",
    "# tuned_parameters = [{'penalty':['l1', '12'], 'C':[0.001, 0.01, 0.1, 1, 10]}]\n",
    "# clf = GridSearchCV(LogisticRegression(), tuned_parameters, cv=5, scoring='roc_auc')\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.best_params_\n",
    "# lr = LogisticRegression(penalty='l2', C=0.1)\n",
    "# lr_clf = lf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a51df-9872-4a3e-85f4-e02683046f37",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "* 筛选特征的方法：基于XGBoost输出的feature_importance对变量进行重要性排序，根据不同的重要性阈值进行筛选，观察每组变量筛选后模型在验证集或者测试集上的效果，选择保持模型效果且入模特征数量较少的阈值\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1ad3cf-c237-4859-a508-104013ae22bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "\n",
    "# X_columns = data_xgb.columns[2:]\n",
    "# Y_columns = ['TARGET']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_xgb[X_columns], data_xgb[Y_columns], test_size=0.3, random_state=0)\n",
    "# X_matrix_train = X_train.as_matrix(columns=None)\n",
    "# Y_matrix_train = y_train.as_matrix(columns=None)\n",
    "# X_matrix_test = X_test.as_matrix(columns=None)\n",
    "# Y_matrix_test = y_test.as_matrix(columns=None)\n",
    "# xgb_clf = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=10, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', seed=0, n_jobs=-1)\n",
    "\n",
    "# print(datetime.now())\n",
    "# xgb_clf.fit(X_matrix_train, Y_matrix_train, eval_metric='auc')\n",
    "# y_pred = xgb_clf.predict_proba(X_matrix_test)[:, 1]\n",
    "# fpr, tpr, threshold = roc_curve(Y_matrix_test, y_pred)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# print('test auc: %f' %roc_auc)\n",
    "# print(datetime.now())\n",
    "\n",
    "# threshold = np.sort(xgb_clf.feature_importances_)\n",
    "# print(datetime.now())\n",
    "# for thresh in threshold[400:700:10]:\n",
    "#     selection = SelectFromModel(xgb_clf, threshold=thresh, prefit=True)\n",
    "#     X_train_selection = selection.transform(X_matrix_train)\n",
    "#     selection_model = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=10, subsample=0.8, colsample_bytree=0.8, \n",
    "#                                     objective='binary:logistic', seed=0, n_jobs=-1)\n",
    "#     selection_model.fit(X_train_selection, Y_matrix_train, eval_metric='auc')\n",
    "#     X_test_selection = selection.transform(X_matrix_test)\n",
    "#     y_pred = selection_model.predict_proba(X_test_selection)[:, 1]\n",
    "#     fpr, tpr, threshold = roc_curve(Y_matrix_test, y_pred)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     print('thresh = %.4f, n=%d, test auc: %f' % (thresh, X_train_selection.shape[1], roc_auc))\n",
    "# print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca358b-4c4c-4510-9893-dba4e5b3bc93",
   "metadata": {},
   "source": [
    "* 贝叶斯优化的优点在于，每次调参都会考虑上一组超参数的信息，迭代次数少，调参效率更高\n",
    "* 针对非凸的损失函数，贝叶斯优化也不容易陷入局部最优，能够得到全局最优的超参数组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a29fa21-b2f6-486a-9239-d40017cd54c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 利用hyperopt对XGBoost模型调参\n",
    "# from hyperopt import fmin, tpe, hp, partial\n",
    "\n",
    "\n",
    "# X_columns = data_xgb.columns[2:]\n",
    "# Y_columns = ['TARGET']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_xgb[X_columns], data_xgb[Y_columns], test_size=0.3, random_state=0)\n",
    "# X_matrix_train = X_train.as_matrix(columns=None)\n",
    "# Y_matrix_train = y_train.as_matrix(columns=None)\n",
    "# X_matrix_test = X_test.as_matrix(columns=None)\n",
    "# Y_matrix_test = y_test.as_matrix(columns=None)\n",
    "# eval_set = [(X_matrix_train, Y_matrix_train), (X_matrix_test, Y_matrix_test)]\n",
    "\n",
    "# # 定义超参数空间\n",
    "# space = {\n",
    "#             'max_depth' : hp.randint('max_depth', 2), \n",
    "#             'n_estimators' : hp.randint('n_estimators', 1500), \n",
    "#             'learning_rate' : hp.randint('learning_rate', 50), \n",
    "#             'min_child_weight' : hp.randint('min_child_weight', 100),\n",
    "#             'subsample' : hp.randint('subsample', 5), \n",
    "#             'colsample_bytree' : hp.randint('colsample_bytree', 5), \n",
    "#             'alpha' : hp.randint('alpha', 1000), \n",
    "#             'lambda' : hp.randint('lambda', 1000)\n",
    "#         }\n",
    "\n",
    "# def argsDict_transform(argsDict, isPrint=False):\n",
    "#     argsDict['max_depth'] = argsDict['max_depth'] + 4\n",
    "#     argsDict['n_estimators'] = argsDict['n_estimators'] + 500\n",
    "#     argsDict['learning_rate'] = argsDict['learning_rate'] * 0.01 + 0.01\n",
    "#     argsDict['min_child_weight'] = argsDict['min_child_weight'] + 1\n",
    "#     argsDict['subsample'] = argsDict['subsample'] * 0.1 + 0.5\n",
    "#     argsDict['colsample_bytree'] = argsDict['colsample_bytree'] * 0.1 + 0.5\n",
    "#     argsDict['alpha'] = argsDict['alpha'] * 0.01\n",
    "#     argsDict['lambda'] = argsDict['lambda'] * 0.01\n",
    "    \n",
    "#     if isPrint:\n",
    "#         print(argsDict)\n",
    "    \n",
    "#     return argsDict\n",
    "\n",
    "# # 定义metric\n",
    "# def aug_neg(y_pred, dtrain):\n",
    "#     y_true = dtrain.get_label()\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "#     auc_neg = -auc(fpr, tpr)\n",
    "#     return 'auc_neg', auc_neg\n",
    "\n",
    "# # 定义训练过程\n",
    "# def xgboost_factory(argsDict):\n",
    "#     argsDict = argsDict_transform(argsDict)\n",
    "#     params = {\n",
    "#                 'max_depth' : argsDict['max_depth'], \n",
    "#                 'n_estimators' : argsDict['n_estimators'], \n",
    "#                 'learning_rate' : argsDict['learning_rate'], \n",
    "#                 'min_child_weight' : argsDict['min_child_weight'],\n",
    "#                 'subsample' : argsDict['subsample'], \n",
    "#                 'colsample_bytree' : argsDict['colsample_bytree'], \n",
    "#                 'objective' : 'binary:logistic',\n",
    "#                 'silent' : True,\n",
    "#                 'alpha' : argsDict['max_depth'], \n",
    "#                 'lambda' : argsDict['max_depth'],\n",
    "#                 'seed' : 0\n",
    "#             }\n",
    "#     xgb_clf = XGBClassifier(**params)\n",
    "#     xgb_clf.fit(X_matrix_train, Y_matrix_train, eval_set=eval_set, eval_metric=auc_neg, early_stopping_rounds=100, verbose=10)\n",
    "#     return get_transformer_score(xgb_clf)\n",
    "\n",
    "# def get_transformer_score(transformer):\n",
    "#     xgb_clf = transformer\n",
    "#     y_pred = xgb_clf.predict_proba(X_matrix_test, ntree_limit=xgb_clf.best_ntree_limit)[:, 1]\n",
    "#     fpr, tpr, threshold = roc_curve(Y_matrix_test, y_pred)\n",
    "#     return -auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6eae62-b02f-4c88-9762-4cf52a689a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09e67a8-7fe6-41a2-9e88-a133bf231c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 搜索最优超参数组合\n",
    "\n",
    "# algo = partial(tpe.suggest, n_startup_jobs=1)\n",
    "# best = fmin(xgboost_factory, space, algo=algo, max_evals=20, pass_expr_memo_ctrl=None)\n",
    "# argsDict = argsDict_transform(best)\n",
    "# params = {\n",
    "#             'max_depth' : argsDict['max_depth'], \n",
    "#             'n_estimators' : argsDict['n_estimators'], \n",
    "#             'learning_rate' : argsDict['learning_rate'], \n",
    "#             'min_child_weight' : argsDict['min_child_weight'],\n",
    "#             'subsample' : argsDict['subsample'], \n",
    "#             'colsample_bytree' : argsDict['colsample_bytree'], \n",
    "#             'objective' : 'binary:logistic',\n",
    "#             'silent' : True,\n",
    "#             'alpha' : argsDict['max_depth'], \n",
    "#             'lambda' : argsDict['max_depth'],\n",
    "#             'seed' : 0\n",
    "#         }\n",
    "\n",
    "# # 加入早停条件\n",
    "# xgb_clf = XGBClassifier(**params)\n",
    "# xgb_clf = xgb_clf.fit(X_matrix_train, Y_matrix_train, eval_set=eval_set, eval_metric=auc_neg, early_stopping_rounds=100, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed72b22-acda-4ff1-bdbd-10221a6972c9",
   "metadata": {},
   "source": [
    "利用SHAP值进行XGBoost模型白盒化，解释入模特征对于目标的贡献度和贡献方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edb060-c1d2-4c64-b495-66d106d881ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_clf)\n",
    "shap_values = explainer.shap_values(data_train[X_columns])\n",
    "shap.summary_plot(shap_values, data_train[X_columns])\n",
    "shap.summary_plot(shap_values, data_train[X_columns], plot_type='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
